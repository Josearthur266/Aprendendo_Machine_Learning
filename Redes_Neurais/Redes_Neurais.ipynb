{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b951cb13",
   "metadata": {},
   "source": [
    "# Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7015b5",
   "metadata": {},
   "source": [
    "## Base credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3876fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5e05440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('credit.pkl', 'rb') as f:\n",
    "    x_credit_treinamento, y_credit_treinamento, x_credit_teste, y_credit_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b7226d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 3), (1500,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_credit_treinamento.shape, y_credit_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f9eacde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 3), (500, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_credit_teste.shape, x_credit_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0e2cd59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.18265109\n",
      "Iteration 2, loss = 1.16866311\n",
      "Iteration 3, loss = 1.15521724\n",
      "Iteration 4, loss = 1.14252735\n",
      "Iteration 5, loss = 1.13021283\n",
      "Iteration 6, loss = 1.11848514\n",
      "Iteration 7, loss = 1.10724566\n",
      "Iteration 8, loss = 1.09656033\n",
      "Iteration 9, loss = 1.08629232\n",
      "Iteration 10, loss = 1.07649159\n",
      "Iteration 11, loss = 1.06728823\n",
      "Iteration 12, loss = 1.05821036\n",
      "Iteration 13, loss = 1.04969930\n",
      "Iteration 14, loss = 1.04140558\n",
      "Iteration 15, loss = 1.03347894\n",
      "Iteration 16, loss = 1.02581332\n",
      "Iteration 17, loss = 1.01847670\n",
      "Iteration 18, loss = 1.01142625\n",
      "Iteration 19, loss = 1.00446118\n",
      "Iteration 20, loss = 0.99788029\n",
      "Iteration 21, loss = 0.99137970\n",
      "Iteration 22, loss = 0.98517996\n",
      "Iteration 23, loss = 0.97915350\n",
      "Iteration 24, loss = 0.97328347\n",
      "Iteration 25, loss = 0.96759915\n",
      "Iteration 26, loss = 0.96206991\n",
      "Iteration 27, loss = 0.95663646\n",
      "Iteration 28, loss = 0.95135508\n",
      "Iteration 29, loss = 0.94621901\n",
      "Iteration 30, loss = 0.94116263\n",
      "Iteration 31, loss = 0.93625591\n",
      "Iteration 32, loss = 0.93139537\n",
      "Iteration 33, loss = 0.92673798\n",
      "Iteration 34, loss = 0.92207483\n",
      "Iteration 35, loss = 0.91744769\n",
      "Iteration 36, loss = 0.91309334\n",
      "Iteration 37, loss = 0.90858749\n",
      "Iteration 38, loss = 0.90426327\n",
      "Iteration 39, loss = 0.90000325\n",
      "Iteration 40, loss = 0.89579505\n",
      "Iteration 41, loss = 0.89161754\n",
      "Iteration 42, loss = 0.88752439\n",
      "Iteration 43, loss = 0.88348669\n",
      "Iteration 44, loss = 0.87949680\n",
      "Iteration 45, loss = 0.87551724\n",
      "Iteration 46, loss = 0.87165043\n",
      "Iteration 47, loss = 0.86775803\n",
      "Iteration 48, loss = 0.86398801\n",
      "Iteration 49, loss = 0.86023256\n",
      "Iteration 50, loss = 0.85646690\n",
      "Iteration 51, loss = 0.85280086\n",
      "Iteration 52, loss = 0.84908320\n",
      "Iteration 53, loss = 0.84546645\n",
      "Iteration 54, loss = 0.84178748\n",
      "Iteration 55, loss = 0.83824213\n",
      "Iteration 56, loss = 0.83462669\n",
      "Iteration 57, loss = 0.83105319\n",
      "Iteration 58, loss = 0.82756800\n",
      "Iteration 59, loss = 0.82407403\n",
      "Iteration 60, loss = 0.82056059\n",
      "Iteration 61, loss = 0.81714205\n",
      "Iteration 62, loss = 0.81369042\n",
      "Iteration 63, loss = 0.81027758\n",
      "Iteration 64, loss = 0.80681448\n",
      "Iteration 65, loss = 0.80343764\n",
      "Iteration 66, loss = 0.80004299\n",
      "Iteration 67, loss = 0.79660370\n",
      "Iteration 68, loss = 0.79325081\n",
      "Iteration 69, loss = 0.78984682\n",
      "Iteration 70, loss = 0.78648798\n",
      "Iteration 71, loss = 0.78308758\n",
      "Iteration 72, loss = 0.77966210\n",
      "Iteration 73, loss = 0.77626737\n",
      "Iteration 74, loss = 0.77282372\n",
      "Iteration 75, loss = 0.76941705\n",
      "Iteration 76, loss = 0.76597276\n",
      "Iteration 77, loss = 0.76247105\n",
      "Iteration 78, loss = 0.75900323\n",
      "Iteration 79, loss = 0.75540968\n",
      "Iteration 80, loss = 0.75184586\n",
      "Iteration 81, loss = 0.74822399\n",
      "Iteration 82, loss = 0.74451060\n",
      "Iteration 83, loss = 0.74083092\n",
      "Iteration 84, loss = 0.73699154\n",
      "Iteration 85, loss = 0.73292663\n",
      "Iteration 86, loss = 0.72886697\n",
      "Iteration 87, loss = 0.72463120\n",
      "Iteration 88, loss = 0.72030066\n",
      "Iteration 89, loss = 0.71564310\n",
      "Iteration 90, loss = 0.71078869\n",
      "Iteration 91, loss = 0.70539445\n",
      "Iteration 92, loss = 0.69962408\n",
      "Iteration 93, loss = 0.69308270\n",
      "Iteration 94, loss = 0.68577255\n",
      "Iteration 95, loss = 0.67786817\n",
      "Iteration 96, loss = 0.66902267\n",
      "Iteration 97, loss = 0.65935971\n",
      "Iteration 98, loss = 0.64878841\n",
      "Iteration 99, loss = 0.63747589\n",
      "Iteration 100, loss = 0.62570025\n",
      "Iteration 101, loss = 0.61412580\n",
      "Iteration 102, loss = 0.60184246\n",
      "Iteration 103, loss = 0.58956841\n",
      "Iteration 104, loss = 0.57702219\n",
      "Iteration 105, loss = 0.56443079\n",
      "Iteration 106, loss = 0.55169011\n",
      "Iteration 107, loss = 0.53940165\n",
      "Iteration 108, loss = 0.52686063\n",
      "Iteration 109, loss = 0.51453130\n",
      "Iteration 110, loss = 0.50242970\n",
      "Iteration 111, loss = 0.49059400\n",
      "Iteration 112, loss = 0.47929799\n",
      "Iteration 113, loss = 0.46820501\n",
      "Iteration 114, loss = 0.45751247\n",
      "Iteration 115, loss = 0.44719917\n",
      "Iteration 116, loss = 0.43745064\n",
      "Iteration 117, loss = 0.42812571\n",
      "Iteration 118, loss = 0.41906592\n",
      "Iteration 119, loss = 0.41060979\n",
      "Iteration 120, loss = 0.40226949\n",
      "Iteration 121, loss = 0.39449805\n",
      "Iteration 122, loss = 0.38712829\n",
      "Iteration 123, loss = 0.38000945\n",
      "Iteration 124, loss = 0.37324778\n",
      "Iteration 125, loss = 0.36679777\n",
      "Iteration 126, loss = 0.36069744\n",
      "Iteration 127, loss = 0.35485154\n",
      "Iteration 128, loss = 0.34914445\n",
      "Iteration 129, loss = 0.34379061\n",
      "Iteration 130, loss = 0.33854896\n",
      "Iteration 131, loss = 0.33363444\n",
      "Iteration 132, loss = 0.32885056\n",
      "Iteration 133, loss = 0.32428308\n",
      "Iteration 134, loss = 0.31990961\n",
      "Iteration 135, loss = 0.31573138\n",
      "Iteration 136, loss = 0.31172869\n",
      "Iteration 137, loss = 0.30797595\n",
      "Iteration 138, loss = 0.30435268\n",
      "Iteration 139, loss = 0.30094883\n",
      "Iteration 140, loss = 0.29763325\n",
      "Iteration 141, loss = 0.29443385\n",
      "Iteration 142, loss = 0.29141740\n",
      "Iteration 143, loss = 0.28847685\n",
      "Iteration 144, loss = 0.28570982\n",
      "Iteration 145, loss = 0.28303905\n",
      "Iteration 146, loss = 0.28042440\n",
      "Iteration 147, loss = 0.27798370\n",
      "Iteration 148, loss = 0.27551868\n",
      "Iteration 149, loss = 0.27319948\n",
      "Iteration 150, loss = 0.27093253\n",
      "Iteration 151, loss = 0.26880218\n",
      "Iteration 152, loss = 0.26682361\n",
      "Iteration 153, loss = 0.26489768\n",
      "Iteration 154, loss = 0.26307028\n",
      "Iteration 155, loss = 0.26125248\n",
      "Iteration 156, loss = 0.25952975\n",
      "Iteration 157, loss = 0.25786170\n",
      "Iteration 158, loss = 0.25628273\n",
      "Iteration 159, loss = 0.25480004\n",
      "Iteration 160, loss = 0.25333548\n",
      "Iteration 161, loss = 0.25193254\n",
      "Iteration 162, loss = 0.25059952\n",
      "Iteration 163, loss = 0.24930966\n",
      "Iteration 164, loss = 0.24804483\n",
      "Iteration 165, loss = 0.24680845\n",
      "Iteration 166, loss = 0.24561914\n",
      "Iteration 167, loss = 0.24445736\n",
      "Iteration 168, loss = 0.24336215\n",
      "Iteration 169, loss = 0.24224028\n",
      "Iteration 170, loss = 0.24115611\n",
      "Iteration 171, loss = 0.24011134\n",
      "Iteration 172, loss = 0.23906712\n",
      "Iteration 173, loss = 0.23804403\n",
      "Iteration 174, loss = 0.23704065\n",
      "Iteration 175, loss = 0.23608162\n",
      "Iteration 176, loss = 0.23512849\n",
      "Iteration 177, loss = 0.23420648\n",
      "Iteration 178, loss = 0.23326608\n",
      "Iteration 179, loss = 0.23237166\n",
      "Iteration 180, loss = 0.23148765\n",
      "Iteration 181, loss = 0.23059956\n",
      "Iteration 182, loss = 0.22973759\n",
      "Iteration 183, loss = 0.22888092\n",
      "Iteration 184, loss = 0.22802745\n",
      "Iteration 185, loss = 0.22717224\n",
      "Iteration 186, loss = 0.22633708\n",
      "Iteration 187, loss = 0.22552313\n",
      "Iteration 188, loss = 0.22472614\n",
      "Iteration 189, loss = 0.22392192\n",
      "Iteration 190, loss = 0.22311936\n",
      "Iteration 191, loss = 0.22235307\n",
      "Iteration 192, loss = 0.22154370\n",
      "Iteration 193, loss = 0.22078661\n",
      "Iteration 194, loss = 0.21999702\n",
      "Iteration 195, loss = 0.21921509\n",
      "Iteration 196, loss = 0.21846291\n",
      "Iteration 197, loss = 0.21770662\n",
      "Iteration 198, loss = 0.21694990\n",
      "Iteration 199, loss = 0.21620846\n",
      "Iteration 200, loss = 0.21545576\n",
      "Iteration 201, loss = 0.21471233\n",
      "Iteration 202, loss = 0.21397638\n",
      "Iteration 203, loss = 0.21323005\n",
      "Iteration 204, loss = 0.21249524\n",
      "Iteration 205, loss = 0.21176825\n",
      "Iteration 206, loss = 0.21103475\n",
      "Iteration 207, loss = 0.21032343\n",
      "Iteration 208, loss = 0.20961208\n",
      "Iteration 209, loss = 0.20889948\n",
      "Iteration 210, loss = 0.20818452\n",
      "Iteration 211, loss = 0.20749654\n",
      "Iteration 212, loss = 0.20676954\n",
      "Iteration 213, loss = 0.20607975\n",
      "Iteration 214, loss = 0.20538483\n",
      "Iteration 215, loss = 0.20471323\n",
      "Iteration 216, loss = 0.20402836\n",
      "Iteration 217, loss = 0.20334109\n",
      "Iteration 218, loss = 0.20266893\n",
      "Iteration 219, loss = 0.20199162\n",
      "Iteration 220, loss = 0.20131528\n",
      "Iteration 221, loss = 0.20067965\n",
      "Iteration 222, loss = 0.19999009\n",
      "Iteration 223, loss = 0.19933708\n",
      "Iteration 224, loss = 0.19867994\n",
      "Iteration 225, loss = 0.19801238\n",
      "Iteration 226, loss = 0.19736610\n",
      "Iteration 227, loss = 0.19671239\n",
      "Iteration 228, loss = 0.19606376\n",
      "Iteration 229, loss = 0.19542676\n",
      "Iteration 230, loss = 0.19479822\n",
      "Iteration 231, loss = 0.19415408\n",
      "Iteration 232, loss = 0.19351403\n",
      "Iteration 233, loss = 0.19287702\n",
      "Iteration 234, loss = 0.19225078\n",
      "Iteration 235, loss = 0.19162378\n",
      "Iteration 236, loss = 0.19099453\n",
      "Iteration 237, loss = 0.19038327\n",
      "Iteration 238, loss = 0.18974606\n",
      "Iteration 239, loss = 0.18915210\n",
      "Iteration 240, loss = 0.18853170\n",
      "Iteration 241, loss = 0.18793072\n",
      "Iteration 242, loss = 0.18731078\n",
      "Iteration 243, loss = 0.18670327\n",
      "Iteration 244, loss = 0.18611736\n",
      "Iteration 245, loss = 0.18551948\n",
      "Iteration 246, loss = 0.18490220\n",
      "Iteration 247, loss = 0.18431419\n",
      "Iteration 248, loss = 0.18372062\n",
      "Iteration 249, loss = 0.18312797\n",
      "Iteration 250, loss = 0.18254797\n",
      "Iteration 251, loss = 0.18195454\n",
      "Iteration 252, loss = 0.18139329\n",
      "Iteration 253, loss = 0.18082538\n",
      "Iteration 254, loss = 0.18025323\n",
      "Iteration 255, loss = 0.17967292\n",
      "Iteration 256, loss = 0.17912143\n",
      "Iteration 257, loss = 0.17854643\n",
      "Iteration 258, loss = 0.17798812\n",
      "Iteration 259, loss = 0.17743807\n",
      "Iteration 260, loss = 0.17690538\n",
      "Iteration 261, loss = 0.17635582\n",
      "Iteration 262, loss = 0.17583913\n",
      "Iteration 263, loss = 0.17526973\n",
      "Iteration 264, loss = 0.17474887\n",
      "Iteration 265, loss = 0.17421626\n",
      "Iteration 266, loss = 0.17368143\n",
      "Iteration 267, loss = 0.17315254\n",
      "Iteration 268, loss = 0.17263186\n",
      "Iteration 269, loss = 0.17209916\n",
      "Iteration 270, loss = 0.17158714\n",
      "Iteration 271, loss = 0.17108438\n",
      "Iteration 272, loss = 0.17059051\n",
      "Iteration 273, loss = 0.17020118\n",
      "Iteration 274, loss = 0.16991753\n",
      "Iteration 275, loss = 0.16963014\n",
      "Iteration 276, loss = 0.16935101\n",
      "Iteration 277, loss = 0.16906829\n",
      "Iteration 278, loss = 0.16878445\n",
      "Iteration 279, loss = 0.16851859\n",
      "Iteration 280, loss = 0.16826800\n",
      "Iteration 281, loss = 0.16798536\n",
      "Iteration 282, loss = 0.16774886\n",
      "Iteration 283, loss = 0.16750406\n",
      "Iteration 284, loss = 0.16724714\n",
      "Iteration 285, loss = 0.16702221\n",
      "Iteration 286, loss = 0.16676824\n",
      "Iteration 287, loss = 0.16654816\n",
      "Iteration 288, loss = 0.16632466\n",
      "Iteration 289, loss = 0.16608494\n",
      "Iteration 290, loss = 0.16588134\n",
      "Iteration 291, loss = 0.16564494\n",
      "Iteration 292, loss = 0.16543494\n",
      "Iteration 293, loss = 0.16522461\n",
      "Iteration 294, loss = 0.16500549\n",
      "Iteration 295, loss = 0.16481117\n",
      "Iteration 296, loss = 0.16458548\n",
      "Iteration 297, loss = 0.16437985\n",
      "Iteration 298, loss = 0.16419082\n",
      "Iteration 299, loss = 0.16399179\n",
      "Iteration 300, loss = 0.16378413\n",
      "Iteration 301, loss = 0.16359831\n",
      "Iteration 302, loss = 0.16340517\n",
      "Iteration 303, loss = 0.16321477\n",
      "Iteration 304, loss = 0.16302323\n",
      "Iteration 305, loss = 0.16283935\n",
      "Iteration 306, loss = 0.16266153\n",
      "Iteration 307, loss = 0.16248544\n",
      "Iteration 308, loss = 0.16229309\n",
      "Iteration 309, loss = 0.16214825\n",
      "Iteration 310, loss = 0.16195657\n",
      "Iteration 311, loss = 0.16175579\n",
      "Iteration 312, loss = 0.16158250\n",
      "Iteration 313, loss = 0.16142044\n",
      "Iteration 314, loss = 0.16123842\n",
      "Iteration 315, loss = 0.16107063\n",
      "Iteration 316, loss = 0.16090004\n",
      "Iteration 317, loss = 0.16073022\n",
      "Iteration 318, loss = 0.16057076\n",
      "Iteration 319, loss = 0.16039269\n",
      "Iteration 320, loss = 0.16023440\n",
      "Iteration 321, loss = 0.16005260\n",
      "Iteration 322, loss = 0.15989055\n",
      "Iteration 323, loss = 0.15972578\n",
      "Iteration 324, loss = 0.15956173\n",
      "Iteration 325, loss = 0.15938590\n",
      "Iteration 326, loss = 0.15922420\n",
      "Iteration 327, loss = 0.15905799\n",
      "Iteration 328, loss = 0.15888999\n",
      "Iteration 329, loss = 0.15875484\n",
      "Iteration 330, loss = 0.15855885\n",
      "Iteration 331, loss = 0.15840443\n",
      "Iteration 332, loss = 0.15825985\n",
      "Iteration 333, loss = 0.15810549\n",
      "Iteration 334, loss = 0.15793746\n",
      "Iteration 335, loss = 0.15782199\n",
      "Iteration 336, loss = 0.15764408\n",
      "Iteration 337, loss = 0.15748199\n",
      "Iteration 338, loss = 0.15733068\n",
      "Iteration 339, loss = 0.15718412\n",
      "Iteration 340, loss = 0.15703672\n",
      "Iteration 341, loss = 0.15687280\n",
      "Iteration 342, loss = 0.15673523\n",
      "Iteration 343, loss = 0.15658848\n",
      "Iteration 344, loss = 0.15643104\n",
      "Iteration 345, loss = 0.15629685\n",
      "Iteration 346, loss = 0.15613429\n",
      "Iteration 347, loss = 0.15600871\n",
      "Iteration 348, loss = 0.15586670\n",
      "Iteration 349, loss = 0.15573162\n",
      "Iteration 350, loss = 0.15555682\n",
      "Iteration 351, loss = 0.15541729\n",
      "Iteration 352, loss = 0.15527728\n",
      "Iteration 353, loss = 0.15513976\n",
      "Iteration 354, loss = 0.15498745\n",
      "Iteration 355, loss = 0.15486725\n",
      "Iteration 356, loss = 0.15471015\n",
      "Iteration 357, loss = 0.15457161\n",
      "Iteration 358, loss = 0.15443764\n",
      "Iteration 359, loss = 0.15430013\n",
      "Iteration 360, loss = 0.15417128\n",
      "Iteration 361, loss = 0.15402906\n",
      "Iteration 362, loss = 0.15389435\n",
      "Iteration 363, loss = 0.15376143\n",
      "Iteration 364, loss = 0.15361781\n",
      "Iteration 365, loss = 0.15348181\n",
      "Iteration 366, loss = 0.15335474\n",
      "Iteration 367, loss = 0.15323922\n",
      "Iteration 368, loss = 0.15310918\n",
      "Iteration 369, loss = 0.15296451\n",
      "Iteration 370, loss = 0.15282741\n",
      "Iteration 371, loss = 0.15269292\n",
      "Iteration 372, loss = 0.15256962\n",
      "Iteration 373, loss = 0.15242705\n",
      "Iteration 374, loss = 0.15232383\n",
      "Iteration 375, loss = 0.15218652\n",
      "Iteration 376, loss = 0.15204499\n",
      "Iteration 377, loss = 0.15192528\n",
      "Iteration 378, loss = 0.15180071\n",
      "Iteration 379, loss = 0.15164227\n",
      "Iteration 380, loss = 0.15150311\n",
      "Iteration 381, loss = 0.15135250\n",
      "Iteration 382, loss = 0.15121744\n",
      "Iteration 383, loss = 0.15107770\n",
      "Iteration 384, loss = 0.15093337\n",
      "Iteration 385, loss = 0.15080253\n",
      "Iteration 386, loss = 0.15065069\n",
      "Iteration 387, loss = 0.15053516\n",
      "Iteration 388, loss = 0.15038134\n",
      "Iteration 389, loss = 0.15025966\n",
      "Iteration 390, loss = 0.15015573\n",
      "Iteration 391, loss = 0.14998160\n",
      "Iteration 392, loss = 0.14987219\n",
      "Iteration 393, loss = 0.14974105\n",
      "Iteration 394, loss = 0.14959043\n",
      "Iteration 395, loss = 0.14945084\n",
      "Iteration 396, loss = 0.14936993\n",
      "Iteration 397, loss = 0.14920078\n",
      "Iteration 398, loss = 0.14907598\n",
      "Iteration 399, loss = 0.14895222\n",
      "Iteration 400, loss = 0.14882857\n",
      "Iteration 401, loss = 0.14869959\n",
      "Iteration 402, loss = 0.14856170\n",
      "Iteration 403, loss = 0.14843895\n",
      "Iteration 404, loss = 0.14831295\n",
      "Iteration 405, loss = 0.14818553\n",
      "Iteration 406, loss = 0.14806192\n",
      "Iteration 407, loss = 0.14797751\n",
      "Iteration 408, loss = 0.14780032\n",
      "Iteration 409, loss = 0.14771602\n",
      "Iteration 410, loss = 0.14759498\n",
      "Iteration 411, loss = 0.14745024\n",
      "Iteration 412, loss = 0.14731181\n",
      "Iteration 413, loss = 0.14719338\n",
      "Iteration 414, loss = 0.14706376\n",
      "Iteration 415, loss = 0.14694233\n",
      "Iteration 416, loss = 0.14681040\n",
      "Iteration 417, loss = 0.14669228\n",
      "Iteration 418, loss = 0.14655959\n",
      "Iteration 419, loss = 0.14643584\n",
      "Iteration 420, loss = 0.14630648\n",
      "Iteration 421, loss = 0.14620800\n",
      "Iteration 422, loss = 0.14607734\n",
      "Iteration 423, loss = 0.14593087\n",
      "Iteration 424, loss = 0.14579424\n",
      "Iteration 425, loss = 0.14566408\n",
      "Iteration 426, loss = 0.14552877\n",
      "Iteration 427, loss = 0.14540654\n",
      "Iteration 428, loss = 0.14530842\n",
      "Iteration 429, loss = 0.14513722\n",
      "Iteration 430, loss = 0.14501899\n",
      "Iteration 431, loss = 0.14488486\n",
      "Iteration 432, loss = 0.14475828\n",
      "Iteration 433, loss = 0.14464794\n",
      "Iteration 434, loss = 0.14450123\n",
      "Iteration 435, loss = 0.14438036\n",
      "Iteration 436, loss = 0.14424538\n",
      "Iteration 437, loss = 0.14414836\n",
      "Iteration 438, loss = 0.14399838\n",
      "Iteration 439, loss = 0.14388188\n",
      "Iteration 440, loss = 0.14374754\n",
      "Iteration 441, loss = 0.14361714\n",
      "Iteration 442, loss = 0.14347276\n",
      "Iteration 443, loss = 0.14336648\n",
      "Iteration 444, loss = 0.14324355\n",
      "Iteration 445, loss = 0.14310273\n",
      "Iteration 446, loss = 0.14299373\n",
      "Iteration 447, loss = 0.14285148\n",
      "Iteration 448, loss = 0.14275004\n",
      "Iteration 449, loss = 0.14260652\n",
      "Iteration 450, loss = 0.14246475\n",
      "Iteration 451, loss = 0.14232560\n",
      "Iteration 452, loss = 0.14222415\n",
      "Iteration 453, loss = 0.14208344\n",
      "Iteration 454, loss = 0.14196583\n",
      "Iteration 455, loss = 0.14183087\n",
      "Iteration 456, loss = 0.14173095\n",
      "Iteration 457, loss = 0.14158406\n",
      "Iteration 458, loss = 0.14145048\n",
      "Iteration 459, loss = 0.14135119\n",
      "Iteration 460, loss = 0.14119716\n",
      "Iteration 461, loss = 0.14107311\n",
      "Iteration 462, loss = 0.14093286\n",
      "Iteration 463, loss = 0.14083534\n",
      "Iteration 464, loss = 0.14067840\n",
      "Iteration 465, loss = 0.14054200\n",
      "Iteration 466, loss = 0.14040000\n",
      "Iteration 467, loss = 0.14027565\n",
      "Iteration 468, loss = 0.14013919\n",
      "Iteration 469, loss = 0.14001288\n",
      "Iteration 470, loss = 0.13986744\n",
      "Iteration 471, loss = 0.13977478\n",
      "Iteration 472, loss = 0.13962033\n",
      "Iteration 473, loss = 0.13949503\n",
      "Iteration 474, loss = 0.13937404\n",
      "Iteration 475, loss = 0.13924279\n",
      "Iteration 476, loss = 0.13909313\n",
      "Iteration 477, loss = 0.13896881\n",
      "Iteration 478, loss = 0.13883675\n",
      "Iteration 479, loss = 0.13870167\n",
      "Iteration 480, loss = 0.13856973\n",
      "Iteration 481, loss = 0.13844425\n",
      "Iteration 482, loss = 0.13835186\n",
      "Iteration 483, loss = 0.13819128\n",
      "Iteration 484, loss = 0.13805420\n",
      "Iteration 485, loss = 0.13791491\n",
      "Iteration 486, loss = 0.13779192\n",
      "Iteration 487, loss = 0.13767584\n",
      "Iteration 488, loss = 0.13755933\n",
      "Iteration 489, loss = 0.13739526\n",
      "Iteration 490, loss = 0.13727891\n",
      "Iteration 491, loss = 0.13715113\n",
      "Iteration 492, loss = 0.13700963\n",
      "Iteration 493, loss = 0.13690968\n",
      "Iteration 494, loss = 0.13675225\n",
      "Iteration 495, loss = 0.13660697\n",
      "Iteration 496, loss = 0.13647626\n",
      "Iteration 497, loss = 0.13633591\n",
      "Iteration 498, loss = 0.13621906\n",
      "Iteration 499, loss = 0.13608204\n",
      "Iteration 500, loss = 0.13593559\n",
      "Iteration 501, loss = 0.13579124\n",
      "Iteration 502, loss = 0.13565959\n",
      "Iteration 503, loss = 0.13551389\n",
      "Iteration 504, loss = 0.13537309\n",
      "Iteration 505, loss = 0.13523677\n",
      "Iteration 506, loss = 0.13508507\n",
      "Iteration 507, loss = 0.13493265\n",
      "Iteration 508, loss = 0.13482241\n",
      "Iteration 509, loss = 0.13465515\n",
      "Iteration 510, loss = 0.13456930\n",
      "Iteration 511, loss = 0.13438055\n",
      "Iteration 512, loss = 0.13422973\n",
      "Iteration 513, loss = 0.13410409\n",
      "Iteration 514, loss = 0.13393925\n",
      "Iteration 515, loss = 0.13380669\n",
      "Iteration 516, loss = 0.13364582\n",
      "Iteration 517, loss = 0.13351946\n",
      "Iteration 518, loss = 0.13335715\n",
      "Iteration 519, loss = 0.13321492\n",
      "Iteration 520, loss = 0.13307609\n",
      "Iteration 521, loss = 0.13292008\n",
      "Iteration 522, loss = 0.13278175\n",
      "Iteration 523, loss = 0.13261289\n",
      "Iteration 524, loss = 0.13245420\n",
      "Iteration 525, loss = 0.13230577\n",
      "Iteration 526, loss = 0.13215363\n",
      "Iteration 527, loss = 0.13199226\n",
      "Iteration 528, loss = 0.13183590\n",
      "Iteration 529, loss = 0.13171864\n",
      "Iteration 530, loss = 0.13157473\n",
      "Iteration 531, loss = 0.13140406\n",
      "Iteration 532, loss = 0.13125120\n",
      "Iteration 533, loss = 0.13111057\n",
      "Iteration 534, loss = 0.13096324\n",
      "Iteration 535, loss = 0.13080814\n",
      "Iteration 536, loss = 0.13065339\n",
      "Iteration 537, loss = 0.13056199\n",
      "Iteration 538, loss = 0.13038262\n",
      "Iteration 539, loss = 0.13021067\n",
      "Iteration 540, loss = 0.13006634\n",
      "Iteration 541, loss = 0.12990400\n",
      "Iteration 542, loss = 0.12978224\n",
      "Iteration 543, loss = 0.12961814\n",
      "Iteration 544, loss = 0.12947073\n",
      "Iteration 545, loss = 0.12934075\n",
      "Iteration 546, loss = 0.12918555\n",
      "Iteration 547, loss = 0.12903631\n",
      "Iteration 548, loss = 0.12889159\n",
      "Iteration 549, loss = 0.12876190\n",
      "Iteration 550, loss = 0.12859165\n",
      "Iteration 551, loss = 0.12844197\n",
      "Iteration 552, loss = 0.12829976\n",
      "Iteration 553, loss = 0.12815587\n",
      "Iteration 554, loss = 0.12802898\n",
      "Iteration 555, loss = 0.12787396\n",
      "Iteration 556, loss = 0.12774248\n",
      "Iteration 557, loss = 0.12760781\n",
      "Iteration 558, loss = 0.12748358\n",
      "Iteration 559, loss = 0.12732133\n",
      "Iteration 560, loss = 0.12718348\n",
      "Iteration 561, loss = 0.12705917\n",
      "Iteration 562, loss = 0.12691685\n",
      "Iteration 563, loss = 0.12677387\n",
      "Iteration 564, loss = 0.12663371\n",
      "Iteration 565, loss = 0.12654974\n",
      "Iteration 566, loss = 0.12636655\n",
      "Iteration 567, loss = 0.12622380\n",
      "Iteration 568, loss = 0.12609504\n",
      "Iteration 569, loss = 0.12595629\n",
      "Iteration 570, loss = 0.12581930\n",
      "Iteration 571, loss = 0.12571379\n",
      "Iteration 572, loss = 0.12553052\n",
      "Iteration 573, loss = 0.12541009\n",
      "Iteration 574, loss = 0.12526209\n",
      "Iteration 575, loss = 0.12514164\n",
      "Iteration 576, loss = 0.12500309\n",
      "Iteration 577, loss = 0.12488628\n",
      "Iteration 578, loss = 0.12474281\n",
      "Iteration 579, loss = 0.12458327\n",
      "Iteration 580, loss = 0.12448947\n",
      "Iteration 581, loss = 0.12438156\n",
      "Iteration 582, loss = 0.12421868\n",
      "Iteration 583, loss = 0.12410217\n",
      "Iteration 584, loss = 0.12394810\n",
      "Iteration 585, loss = 0.12381591\n",
      "Iteration 586, loss = 0.12369453\n",
      "Iteration 587, loss = 0.12357068\n",
      "Iteration 588, loss = 0.12340729\n",
      "Iteration 589, loss = 0.12333926\n",
      "Iteration 590, loss = 0.12317984\n",
      "Iteration 591, loss = 0.12304120\n",
      "Iteration 592, loss = 0.12292423\n",
      "Iteration 593, loss = 0.12282651\n",
      "Iteration 594, loss = 0.12263836\n",
      "Iteration 595, loss = 0.12252073\n",
      "Iteration 596, loss = 0.12242103\n",
      "Iteration 597, loss = 0.12227778\n",
      "Iteration 598, loss = 0.12214677\n",
      "Iteration 599, loss = 0.12199765\n",
      "Iteration 600, loss = 0.12188120\n",
      "Iteration 601, loss = 0.12173069\n",
      "Iteration 602, loss = 0.12162562\n",
      "Iteration 603, loss = 0.12150874\n",
      "Iteration 604, loss = 0.12145141\n",
      "Iteration 605, loss = 0.12128046\n",
      "Iteration 606, loss = 0.12112374\n",
      "Iteration 607, loss = 0.12104151\n",
      "Iteration 608, loss = 0.12090293\n",
      "Iteration 609, loss = 0.12080916\n",
      "Iteration 610, loss = 0.12065389\n",
      "Iteration 611, loss = 0.12054423\n",
      "Iteration 612, loss = 0.12043414\n",
      "Iteration 613, loss = 0.12031274\n",
      "Iteration 614, loss = 0.12019969\n",
      "Iteration 615, loss = 0.12005557\n",
      "Iteration 616, loss = 0.11994970\n",
      "Iteration 617, loss = 0.11981945\n",
      "Iteration 618, loss = 0.11970656\n",
      "Iteration 619, loss = 0.11960815\n",
      "Iteration 620, loss = 0.11946540\n",
      "Iteration 621, loss = 0.11938148\n",
      "Iteration 622, loss = 0.11926320\n",
      "Iteration 623, loss = 0.11913325\n",
      "Iteration 624, loss = 0.11901574\n",
      "Iteration 625, loss = 0.11887139\n",
      "Iteration 626, loss = 0.11877838\n",
      "Iteration 627, loss = 0.11868327\n",
      "Iteration 628, loss = 0.11852530\n",
      "Iteration 629, loss = 0.11848154\n",
      "Iteration 630, loss = 0.11829659\n",
      "Iteration 631, loss = 0.11818679\n",
      "Iteration 632, loss = 0.11805771\n",
      "Iteration 633, loss = 0.11794179\n",
      "Iteration 634, loss = 0.11782566\n",
      "Iteration 635, loss = 0.11771497\n",
      "Iteration 636, loss = 0.11761253\n",
      "Iteration 637, loss = 0.11749511\n",
      "Iteration 638, loss = 0.11737575\n",
      "Iteration 639, loss = 0.11723772\n",
      "Iteration 640, loss = 0.11711700\n",
      "Iteration 641, loss = 0.11697296\n",
      "Iteration 642, loss = 0.11696557\n",
      "Iteration 643, loss = 0.11671272\n",
      "Iteration 644, loss = 0.11656667\n",
      "Iteration 645, loss = 0.11643948\n",
      "Iteration 646, loss = 0.11628566\n",
      "Iteration 647, loss = 0.11616423\n",
      "Iteration 648, loss = 0.11604156\n",
      "Iteration 649, loss = 0.11590759\n",
      "Iteration 650, loss = 0.11580147\n",
      "Iteration 651, loss = 0.11562572\n",
      "Iteration 652, loss = 0.11548470\n",
      "Iteration 653, loss = 0.11533766\n",
      "Iteration 654, loss = 0.11520053\n",
      "Iteration 655, loss = 0.11507666\n",
      "Iteration 656, loss = 0.11490814\n",
      "Iteration 657, loss = 0.11476174\n",
      "Iteration 658, loss = 0.11461316\n",
      "Iteration 659, loss = 0.11445836\n",
      "Iteration 660, loss = 0.11434542\n",
      "Iteration 661, loss = 0.11420555\n",
      "Iteration 662, loss = 0.11399239\n",
      "Iteration 663, loss = 0.11391803\n",
      "Iteration 664, loss = 0.11371592\n",
      "Iteration 665, loss = 0.11353902\n",
      "Iteration 666, loss = 0.11336126\n",
      "Iteration 667, loss = 0.11318242\n",
      "Iteration 668, loss = 0.11300145\n",
      "Iteration 669, loss = 0.11284184\n",
      "Iteration 670, loss = 0.11265833\n",
      "Iteration 671, loss = 0.11247785\n",
      "Iteration 672, loss = 0.11230708\n",
      "Iteration 673, loss = 0.11210237\n",
      "Iteration 674, loss = 0.11194671\n",
      "Iteration 675, loss = 0.11176886\n",
      "Iteration 676, loss = 0.11150727\n",
      "Iteration 677, loss = 0.11127700\n",
      "Iteration 678, loss = 0.11108621\n",
      "Iteration 679, loss = 0.11081410\n",
      "Iteration 680, loss = 0.11057443\n",
      "Iteration 681, loss = 0.11034472\n",
      "Iteration 682, loss = 0.11012672\n",
      "Iteration 683, loss = 0.10993178\n",
      "Iteration 684, loss = 0.10960040\n",
      "Iteration 685, loss = 0.10934235\n",
      "Iteration 686, loss = 0.10908622\n",
      "Iteration 687, loss = 0.10884404\n",
      "Iteration 688, loss = 0.10858596\n",
      "Iteration 689, loss = 0.10827873\n",
      "Iteration 690, loss = 0.10797683\n",
      "Iteration 691, loss = 0.10764291\n",
      "Iteration 692, loss = 0.10735627\n",
      "Iteration 693, loss = 0.10705670\n",
      "Iteration 694, loss = 0.10673277\n",
      "Iteration 695, loss = 0.10632628\n",
      "Iteration 696, loss = 0.10588379\n",
      "Iteration 697, loss = 0.10541899\n",
      "Iteration 698, loss = 0.10493484\n",
      "Iteration 699, loss = 0.10442365\n",
      "Iteration 700, loss = 0.10386734\n",
      "Iteration 701, loss = 0.10330396\n",
      "Iteration 702, loss = 0.10269061\n",
      "Iteration 703, loss = 0.10204536\n",
      "Iteration 704, loss = 0.10146067\n",
      "Iteration 705, loss = 0.10079546\n",
      "Iteration 706, loss = 0.10017875\n",
      "Iteration 707, loss = 0.09959979\n",
      "Iteration 708, loss = 0.09895903\n",
      "Iteration 709, loss = 0.09830482\n",
      "Iteration 710, loss = 0.09752272\n",
      "Iteration 711, loss = 0.09664787\n",
      "Iteration 712, loss = 0.09576981\n",
      "Iteration 713, loss = 0.09504693\n",
      "Iteration 714, loss = 0.09422696\n",
      "Iteration 715, loss = 0.09355979\n",
      "Iteration 716, loss = 0.09285211\n",
      "Iteration 717, loss = 0.09224732\n",
      "Iteration 718, loss = 0.09161658\n",
      "Iteration 719, loss = 0.09104832\n",
      "Iteration 720, loss = 0.09051342\n",
      "Iteration 721, loss = 0.08995464\n",
      "Iteration 722, loss = 0.08946714\n",
      "Iteration 723, loss = 0.08903114\n",
      "Iteration 724, loss = 0.08851530\n",
      "Iteration 725, loss = 0.08814193\n",
      "Iteration 726, loss = 0.08769248\n",
      "Iteration 727, loss = 0.08733557\n",
      "Iteration 728, loss = 0.08693977\n",
      "Iteration 729, loss = 0.08658063\n",
      "Iteration 730, loss = 0.08623139\n",
      "Iteration 731, loss = 0.08587517\n",
      "Iteration 732, loss = 0.08550668\n",
      "Iteration 733, loss = 0.08520113\n",
      "Iteration 734, loss = 0.08485232\n",
      "Iteration 735, loss = 0.08451664\n",
      "Iteration 736, loss = 0.08415746\n",
      "Iteration 737, loss = 0.08383592\n",
      "Iteration 738, loss = 0.08357207\n",
      "Iteration 739, loss = 0.08319158\n",
      "Iteration 740, loss = 0.08292380\n",
      "Iteration 741, loss = 0.08258073\n",
      "Iteration 742, loss = 0.08228452\n",
      "Iteration 743, loss = 0.08199609\n",
      "Iteration 744, loss = 0.08167170\n",
      "Iteration 745, loss = 0.08139296\n",
      "Iteration 746, loss = 0.08109896\n",
      "Iteration 747, loss = 0.08080770\n",
      "Iteration 748, loss = 0.08051653\n",
      "Iteration 749, loss = 0.08024502\n",
      "Iteration 750, loss = 0.07999030\n",
      "Iteration 751, loss = 0.07969536\n",
      "Iteration 752, loss = 0.07941721\n",
      "Iteration 753, loss = 0.07914414\n",
      "Iteration 754, loss = 0.07890332\n",
      "Iteration 755, loss = 0.07861304\n",
      "Iteration 756, loss = 0.07835383\n",
      "Iteration 757, loss = 0.07808282\n",
      "Iteration 758, loss = 0.07782783\n",
      "Iteration 759, loss = 0.07758494\n",
      "Iteration 760, loss = 0.07732649\n",
      "Iteration 761, loss = 0.07703925\n",
      "Iteration 762, loss = 0.07679971\n",
      "Iteration 763, loss = 0.07653557\n",
      "Iteration 764, loss = 0.07630244\n",
      "Iteration 765, loss = 0.07605117\n",
      "Iteration 766, loss = 0.07580190\n",
      "Iteration 767, loss = 0.07555092\n",
      "Iteration 768, loss = 0.07533398\n",
      "Iteration 769, loss = 0.07507718\n",
      "Iteration 770, loss = 0.07484781\n",
      "Iteration 771, loss = 0.07460649\n",
      "Iteration 772, loss = 0.07438544\n",
      "Iteration 773, loss = 0.07413017\n",
      "Iteration 774, loss = 0.07392028\n",
      "Iteration 775, loss = 0.07367042\n",
      "Iteration 776, loss = 0.07347728\n",
      "Iteration 777, loss = 0.07322120\n",
      "Iteration 778, loss = 0.07299537\n",
      "Iteration 779, loss = 0.07279893\n",
      "Iteration 780, loss = 0.07254879\n",
      "Iteration 781, loss = 0.07236201\n",
      "Iteration 782, loss = 0.07212448\n",
      "Iteration 783, loss = 0.07191534\n",
      "Iteration 784, loss = 0.07169089\n",
      "Iteration 785, loss = 0.07146916\n",
      "Iteration 786, loss = 0.07124413\n",
      "Iteration 787, loss = 0.07104307\n",
      "Iteration 788, loss = 0.07083673\n",
      "Iteration 789, loss = 0.07061580\n",
      "Iteration 790, loss = 0.07041454\n",
      "Iteration 791, loss = 0.07020498\n",
      "Iteration 792, loss = 0.07000869\n",
      "Iteration 793, loss = 0.06977897\n",
      "Iteration 794, loss = 0.06956720\n",
      "Iteration 795, loss = 0.06937825\n",
      "Iteration 796, loss = 0.06914856\n",
      "Iteration 797, loss = 0.06892574\n",
      "Iteration 798, loss = 0.06872310\n",
      "Iteration 799, loss = 0.06849025\n",
      "Iteration 800, loss = 0.06826510\n",
      "Iteration 801, loss = 0.06805741\n",
      "Iteration 802, loss = 0.06782979\n",
      "Iteration 803, loss = 0.06758566\n",
      "Iteration 804, loss = 0.06736129\n",
      "Iteration 805, loss = 0.06713721\n",
      "Iteration 806, loss = 0.06692717\n",
      "Iteration 807, loss = 0.06671047\n",
      "Iteration 808, loss = 0.06651434\n",
      "Iteration 809, loss = 0.06627661\n",
      "Iteration 810, loss = 0.06606699\n",
      "Iteration 811, loss = 0.06588007\n",
      "Iteration 812, loss = 0.06565670\n",
      "Iteration 813, loss = 0.06545244\n",
      "Iteration 814, loss = 0.06524724\n",
      "Iteration 815, loss = 0.06502652\n",
      "Iteration 816, loss = 0.06480080\n",
      "Iteration 817, loss = 0.06457913\n",
      "Iteration 818, loss = 0.06434555\n",
      "Iteration 819, loss = 0.06410346\n",
      "Iteration 820, loss = 0.06387407\n",
      "Iteration 821, loss = 0.06366475\n",
      "Iteration 822, loss = 0.06345480\n",
      "Iteration 823, loss = 0.06322162\n",
      "Iteration 824, loss = 0.06299396\n",
      "Iteration 825, loss = 0.06277397\n",
      "Iteration 826, loss = 0.06259507\n",
      "Iteration 827, loss = 0.06236542\n",
      "Iteration 828, loss = 0.06214417\n",
      "Iteration 829, loss = 0.06195546\n",
      "Iteration 830, loss = 0.06172179\n",
      "Iteration 831, loss = 0.06150566\n",
      "Iteration 832, loss = 0.06129687\n",
      "Iteration 833, loss = 0.06107880\n",
      "Iteration 834, loss = 0.06081930\n",
      "Iteration 835, loss = 0.06060269\n",
      "Iteration 836, loss = 0.06038181\n",
      "Iteration 837, loss = 0.06015125\n",
      "Iteration 838, loss = 0.05995612\n",
      "Iteration 839, loss = 0.05972424\n",
      "Iteration 840, loss = 0.05950298\n",
      "Iteration 841, loss = 0.05930652\n",
      "Iteration 842, loss = 0.05906207\n",
      "Iteration 843, loss = 0.05889245\n",
      "Iteration 844, loss = 0.05864851\n",
      "Iteration 845, loss = 0.05844909\n",
      "Iteration 846, loss = 0.05824599\n",
      "Iteration 847, loss = 0.05804525\n",
      "Iteration 848, loss = 0.05781677\n",
      "Iteration 849, loss = 0.05760794\n",
      "Iteration 850, loss = 0.05739229\n",
      "Iteration 851, loss = 0.05715511\n",
      "Iteration 852, loss = 0.05697855\n",
      "Iteration 853, loss = 0.05672236\n",
      "Iteration 854, loss = 0.05658533\n",
      "Iteration 855, loss = 0.05629529\n",
      "Iteration 856, loss = 0.05608074\n",
      "Iteration 857, loss = 0.05588076\n",
      "Iteration 858, loss = 0.05568382\n",
      "Iteration 859, loss = 0.05547306\n",
      "Iteration 860, loss = 0.05526574\n",
      "Iteration 861, loss = 0.05505689\n",
      "Iteration 862, loss = 0.05487279\n",
      "Iteration 863, loss = 0.05463642\n",
      "Iteration 864, loss = 0.05444906\n",
      "Iteration 865, loss = 0.05426018\n",
      "Iteration 866, loss = 0.05406694\n",
      "Iteration 867, loss = 0.05386310\n",
      "Iteration 868, loss = 0.05365253\n",
      "Iteration 869, loss = 0.05348870\n",
      "Iteration 870, loss = 0.05329822\n",
      "Iteration 871, loss = 0.05310066\n",
      "Iteration 872, loss = 0.05292597\n",
      "Iteration 873, loss = 0.05272058\n",
      "Iteration 874, loss = 0.05252944\n",
      "Iteration 875, loss = 0.05233498\n",
      "Iteration 876, loss = 0.05219337\n",
      "Iteration 877, loss = 0.05199833\n",
      "Iteration 878, loss = 0.05178815\n",
      "Iteration 879, loss = 0.05160565\n",
      "Iteration 880, loss = 0.05143519\n",
      "Iteration 881, loss = 0.05125537\n",
      "Iteration 882, loss = 0.05107566\n",
      "Iteration 883, loss = 0.05088961\n",
      "Iteration 884, loss = 0.05071088\n",
      "Iteration 885, loss = 0.05056682\n",
      "Iteration 886, loss = 0.05038735\n",
      "Iteration 887, loss = 0.05017834\n",
      "Iteration 888, loss = 0.05003774\n",
      "Iteration 889, loss = 0.04989516\n",
      "Iteration 890, loss = 0.04969801\n",
      "Iteration 891, loss = 0.04953876\n",
      "Iteration 892, loss = 0.04937222\n",
      "Iteration 893, loss = 0.04921994\n",
      "Iteration 894, loss = 0.04901630\n",
      "Iteration 895, loss = 0.04889081\n",
      "Iteration 896, loss = 0.04872317\n",
      "Iteration 897, loss = 0.04855787\n",
      "Iteration 898, loss = 0.04838765\n",
      "Iteration 899, loss = 0.04822173\n",
      "Iteration 900, loss = 0.04808661\n",
      "Iteration 901, loss = 0.04791204\n",
      "Iteration 902, loss = 0.04773825\n",
      "Iteration 903, loss = 0.04759658\n",
      "Iteration 904, loss = 0.04743112\n",
      "Iteration 905, loss = 0.04725185\n",
      "Iteration 906, loss = 0.04712679\n",
      "Iteration 907, loss = 0.04697990\n",
      "Iteration 908, loss = 0.04678591\n",
      "Iteration 909, loss = 0.04664011\n",
      "Iteration 910, loss = 0.04648094\n",
      "Iteration 911, loss = 0.04632919\n",
      "Iteration 912, loss = 0.04618983\n",
      "Iteration 913, loss = 0.04603144\n",
      "Iteration 914, loss = 0.04587600\n",
      "Iteration 915, loss = 0.04574479\n",
      "Iteration 916, loss = 0.04558932\n",
      "Iteration 917, loss = 0.04541726\n",
      "Iteration 918, loss = 0.04526498\n",
      "Iteration 919, loss = 0.04514247\n",
      "Iteration 920, loss = 0.04499413\n",
      "Iteration 921, loss = 0.04483541\n",
      "Iteration 922, loss = 0.04467056\n",
      "Iteration 923, loss = 0.04453991\n",
      "Iteration 924, loss = 0.04444604\n",
      "Iteration 925, loss = 0.04427571\n",
      "Iteration 926, loss = 0.04410888\n",
      "Iteration 927, loss = 0.04396463\n",
      "Iteration 928, loss = 0.04381890\n",
      "Iteration 929, loss = 0.04369682\n",
      "Iteration 930, loss = 0.04351628\n",
      "Iteration 931, loss = 0.04336689\n",
      "Iteration 932, loss = 0.04319941\n",
      "Iteration 933, loss = 0.04304479\n",
      "Iteration 934, loss = 0.04287776\n",
      "Iteration 935, loss = 0.04274290\n",
      "Iteration 936, loss = 0.04259866\n",
      "Iteration 937, loss = 0.04243209\n",
      "Iteration 938, loss = 0.04228274\n",
      "Iteration 939, loss = 0.04213775\n",
      "Iteration 940, loss = 0.04199893\n",
      "Iteration 941, loss = 0.04184227\n",
      "Iteration 942, loss = 0.04174135\n",
      "Iteration 943, loss = 0.04153627\n",
      "Iteration 944, loss = 0.04136200\n",
      "Iteration 945, loss = 0.04122389\n",
      "Iteration 946, loss = 0.04106883\n",
      "Iteration 947, loss = 0.04093789\n",
      "Iteration 948, loss = 0.04078324\n",
      "Iteration 949, loss = 0.04064985\n",
      "Iteration 950, loss = 0.04047895\n",
      "Iteration 951, loss = 0.04033031\n",
      "Iteration 952, loss = 0.04020226\n",
      "Iteration 953, loss = 0.04007484\n",
      "Iteration 954, loss = 0.03989586\n",
      "Iteration 955, loss = 0.03977048\n",
      "Iteration 956, loss = 0.03961653\n",
      "Iteration 957, loss = 0.03947420\n",
      "Iteration 958, loss = 0.03932779\n",
      "Iteration 959, loss = 0.03918707\n",
      "Iteration 960, loss = 0.03905549\n",
      "Iteration 961, loss = 0.03897673\n",
      "Iteration 962, loss = 0.03877066\n",
      "Iteration 963, loss = 0.03861923\n",
      "Iteration 964, loss = 0.03851406\n",
      "Iteration 965, loss = 0.03837858\n",
      "Iteration 966, loss = 0.03822576\n",
      "Iteration 967, loss = 0.03809002\n",
      "Iteration 968, loss = 0.03793564\n",
      "Iteration 969, loss = 0.03781560\n",
      "Iteration 970, loss = 0.03766638\n",
      "Iteration 971, loss = 0.03754301\n",
      "Iteration 972, loss = 0.03741307\n",
      "Iteration 973, loss = 0.03725947\n",
      "Iteration 974, loss = 0.03712280\n",
      "Iteration 975, loss = 0.03698975\n",
      "Iteration 976, loss = 0.03685160\n",
      "Iteration 977, loss = 0.03669899\n",
      "Iteration 978, loss = 0.03657534\n",
      "Iteration 979, loss = 0.03643779\n",
      "Iteration 980, loss = 0.03630278\n",
      "Iteration 981, loss = 0.03620224\n",
      "Iteration 982, loss = 0.03603953\n",
      "Iteration 983, loss = 0.03591397\n",
      "Iteration 984, loss = 0.03574373\n",
      "Iteration 985, loss = 0.03561767\n",
      "Iteration 986, loss = 0.03549306\n",
      "Iteration 987, loss = 0.03534231\n",
      "Iteration 988, loss = 0.03521829\n",
      "Iteration 989, loss = 0.03507838\n",
      "Iteration 990, loss = 0.03495551\n",
      "Iteration 991, loss = 0.03483275\n",
      "Iteration 992, loss = 0.03469266\n",
      "Iteration 993, loss = 0.03458540\n",
      "Iteration 994, loss = 0.03443270\n",
      "Iteration 995, loss = 0.03432714\n",
      "Iteration 996, loss = 0.03417035\n",
      "Iteration 997, loss = 0.03407237\n",
      "Iteration 998, loss = 0.03396316\n",
      "Iteration 999, loss = 0.03379112\n",
      "Iteration 1000, loss = 0.03367007\n",
      "Iteration 1001, loss = 0.03356725\n",
      "Iteration 1002, loss = 0.03340899\n",
      "Iteration 1003, loss = 0.03328491\n",
      "Iteration 1004, loss = 0.03317631\n",
      "Iteration 1005, loss = 0.03304389\n",
      "Iteration 1006, loss = 0.03292170\n",
      "Iteration 1007, loss = 0.03279689\n",
      "Iteration 1008, loss = 0.03267486\n",
      "Iteration 1009, loss = 0.03254674\n",
      "Iteration 1010, loss = 0.03242411\n",
      "Iteration 1011, loss = 0.03229812\n",
      "Iteration 1012, loss = 0.03223139\n",
      "Iteration 1013, loss = 0.03205940\n",
      "Iteration 1014, loss = 0.03194314\n",
      "Iteration 1015, loss = 0.03181663\n",
      "Iteration 1016, loss = 0.03170901\n",
      "Iteration 1017, loss = 0.03158746\n",
      "Iteration 1018, loss = 0.03148645\n",
      "Iteration 1019, loss = 0.03135165\n",
      "Iteration 1020, loss = 0.03124624\n",
      "Iteration 1021, loss = 0.03112458\n",
      "Iteration 1022, loss = 0.03099897\n",
      "Iteration 1023, loss = 0.03088872\n",
      "Iteration 1024, loss = 0.03077126\n",
      "Iteration 1025, loss = 0.03065502\n",
      "Iteration 1026, loss = 0.03053867\n",
      "Iteration 1027, loss = 0.03040850\n",
      "Iteration 1028, loss = 0.03033271\n",
      "Iteration 1029, loss = 0.03021372\n",
      "Iteration 1030, loss = 0.03009292\n",
      "Iteration 1031, loss = 0.02999536\n",
      "Iteration 1032, loss = 0.02985521\n",
      "Iteration 1033, loss = 0.02976958\n",
      "Iteration 1034, loss = 0.02964604\n",
      "Iteration 1035, loss = 0.02953336\n",
      "Iteration 1036, loss = 0.02940447\n",
      "Iteration 1037, loss = 0.02930026\n",
      "Iteration 1038, loss = 0.02919715\n",
      "Iteration 1039, loss = 0.02907305\n",
      "Iteration 1040, loss = 0.02896536\n",
      "Iteration 1041, loss = 0.02885276\n",
      "Iteration 1042, loss = 0.02874475\n",
      "Iteration 1043, loss = 0.02862663\n",
      "Iteration 1044, loss = 0.02852243\n",
      "Iteration 1045, loss = 0.02846582\n",
      "Iteration 1046, loss = 0.02835043\n",
      "Iteration 1047, loss = 0.02820764\n",
      "Iteration 1048, loss = 0.02810255\n",
      "Iteration 1049, loss = 0.02799801\n",
      "Iteration 1050, loss = 0.02789913\n",
      "Iteration 1051, loss = 0.02777229\n",
      "Iteration 1052, loss = 0.02769154\n",
      "Iteration 1053, loss = 0.02757728\n",
      "Iteration 1054, loss = 0.02749191\n",
      "Iteration 1055, loss = 0.02737936\n",
      "Iteration 1056, loss = 0.02726922\n",
      "Iteration 1057, loss = 0.02716774\n",
      "Iteration 1058, loss = 0.02707010\n",
      "Iteration 1059, loss = 0.02695710\n",
      "Iteration 1060, loss = 0.02687359\n",
      "Iteration 1061, loss = 0.02679442\n",
      "Iteration 1062, loss = 0.02667635\n",
      "Iteration 1063, loss = 0.02657637\n",
      "Iteration 1064, loss = 0.02647710\n",
      "Iteration 1065, loss = 0.02638912\n",
      "Iteration 1066, loss = 0.02627708\n",
      "Iteration 1067, loss = 0.02619298\n",
      "Iteration 1068, loss = 0.02608366\n",
      "Iteration 1069, loss = 0.02596776\n",
      "Iteration 1070, loss = 0.02588677\n",
      "Iteration 1071, loss = 0.02580144\n",
      "Iteration 1072, loss = 0.02568946\n",
      "Iteration 1073, loss = 0.02561360\n",
      "Iteration 1074, loss = 0.02552482\n",
      "Iteration 1075, loss = 0.02541153\n",
      "Iteration 1076, loss = 0.02533300\n",
      "Iteration 1077, loss = 0.02527360\n",
      "Iteration 1078, loss = 0.02514310\n",
      "Iteration 1079, loss = 0.02504729\n",
      "Iteration 1080, loss = 0.02496382\n",
      "Iteration 1081, loss = 0.02486305\n",
      "Iteration 1082, loss = 0.02475767\n",
      "Iteration 1083, loss = 0.02466301\n",
      "Iteration 1084, loss = 0.02459728\n",
      "Iteration 1085, loss = 0.02453663\n",
      "Iteration 1086, loss = 0.02442454\n",
      "Iteration 1087, loss = 0.02431819\n",
      "Iteration 1088, loss = 0.02428636\n",
      "Iteration 1089, loss = 0.02414225\n",
      "Iteration 1090, loss = 0.02405862\n",
      "Iteration 1091, loss = 0.02398494\n",
      "Iteration 1092, loss = 0.02387828\n",
      "Iteration 1093, loss = 0.02378254\n",
      "Iteration 1094, loss = 0.02371063\n",
      "Iteration 1095, loss = 0.02362138\n",
      "Iteration 1096, loss = 0.02353750\n",
      "Iteration 1097, loss = 0.02343990\n",
      "Iteration 1098, loss = 0.02336545\n",
      "Iteration 1099, loss = 0.02327150\n",
      "Iteration 1100, loss = 0.02318048\n",
      "Iteration 1101, loss = 0.02309345\n",
      "Iteration 1102, loss = 0.02305582\n",
      "Iteration 1103, loss = 0.02297044\n",
      "Iteration 1104, loss = 0.02286204\n",
      "Iteration 1105, loss = 0.02279083\n",
      "Iteration 1106, loss = 0.02273172\n",
      "Iteration 1107, loss = 0.02262502\n",
      "Iteration 1108, loss = 0.02255518\n",
      "Iteration 1109, loss = 0.02245586\n",
      "Iteration 1110, loss = 0.02238908\n",
      "Iteration 1111, loss = 0.02229105\n",
      "Iteration 1112, loss = 0.02222337\n",
      "Iteration 1113, loss = 0.02217759\n",
      "Iteration 1114, loss = 0.02204615\n",
      "Iteration 1115, loss = 0.02196244\n",
      "Iteration 1116, loss = 0.02189387\n",
      "Iteration 1117, loss = 0.02180060\n",
      "Iteration 1118, loss = 0.02172167\n",
      "Iteration 1119, loss = 0.02164512\n",
      "Iteration 1120, loss = 0.02158768\n",
      "Iteration 1121, loss = 0.02152373\n",
      "Iteration 1122, loss = 0.02142871\n",
      "Iteration 1123, loss = 0.02140740\n",
      "Iteration 1124, loss = 0.02127673\n",
      "Iteration 1125, loss = 0.02120187\n",
      "Iteration 1126, loss = 0.02111515\n",
      "Iteration 1127, loss = 0.02104686\n",
      "Iteration 1128, loss = 0.02096482\n",
      "Iteration 1129, loss = 0.02089692\n",
      "Iteration 1130, loss = 0.02082622\n",
      "Iteration 1131, loss = 0.02075192\n",
      "Iteration 1132, loss = 0.02068197\n",
      "Iteration 1133, loss = 0.02060614\n",
      "Iteration 1134, loss = 0.02053864\n",
      "Iteration 1135, loss = 0.02047005\n",
      "Iteration 1136, loss = 0.02041269\n",
      "Iteration 1137, loss = 0.02032206\n",
      "Iteration 1138, loss = 0.02024951\n",
      "Iteration 1139, loss = 0.02019199\n",
      "Iteration 1140, loss = 0.02013624\n",
      "Iteration 1141, loss = 0.02008346\n",
      "Iteration 1142, loss = 0.01997517\n",
      "Iteration 1143, loss = 0.01990414\n",
      "Iteration 1144, loss = 0.01984435\n",
      "Iteration 1145, loss = 0.01976828\n",
      "Iteration 1146, loss = 0.01970245\n",
      "Iteration 1147, loss = 0.01962266\n",
      "Iteration 1148, loss = 0.01956572\n",
      "Iteration 1149, loss = 0.01950458\n",
      "Iteration 1150, loss = 0.01943539\n",
      "Iteration 1151, loss = 0.01936687\n",
      "Iteration 1152, loss = 0.01929259\n",
      "Iteration 1153, loss = 0.01923035\n",
      "Iteration 1154, loss = 0.01916958\n",
      "Iteration 1155, loss = 0.01910605\n",
      "Iteration 1156, loss = 0.01903274\n",
      "Iteration 1157, loss = 0.01898087\n",
      "Iteration 1158, loss = 0.01890758\n",
      "Iteration 1159, loss = 0.01885469\n",
      "Iteration 1160, loss = 0.01878239\n",
      "Iteration 1161, loss = 0.01871708\n",
      "Iteration 1162, loss = 0.01864033\n",
      "Iteration 1163, loss = 0.01861112\n",
      "Iteration 1164, loss = 0.01853030\n",
      "Iteration 1165, loss = 0.01846962\n",
      "Iteration 1166, loss = 0.01842267\n",
      "Iteration 1167, loss = 0.01830694\n",
      "Iteration 1168, loss = 0.01824595\n",
      "Iteration 1169, loss = 0.01817045\n",
      "Iteration 1170, loss = 0.01811217\n",
      "Iteration 1171, loss = 0.01806982\n",
      "Iteration 1172, loss = 0.01797515\n",
      "Iteration 1173, loss = 0.01794532\n",
      "Iteration 1174, loss = 0.01786306\n",
      "Iteration 1175, loss = 0.01782122\n",
      "Iteration 1176, loss = 0.01775709\n",
      "Iteration 1177, loss = 0.01768376\n",
      "Iteration 1178, loss = 0.01762763\n",
      "Iteration 1179, loss = 0.01762671\n",
      "Iteration 1180, loss = 0.01753093\n",
      "Iteration 1181, loss = 0.01742756\n",
      "Iteration 1182, loss = 0.01736870\n",
      "Iteration 1183, loss = 0.01730960\n",
      "Iteration 1184, loss = 0.01726995\n",
      "Iteration 1185, loss = 0.01720020\n",
      "Iteration 1186, loss = 0.01714828\n",
      "Iteration 1187, loss = 0.01708410\n",
      "Iteration 1188, loss = 0.01701576\n",
      "Iteration 1189, loss = 0.01698550\n",
      "Iteration 1190, loss = 0.01688875\n",
      "Iteration 1191, loss = 0.01686066\n",
      "Iteration 1192, loss = 0.01681573\n",
      "Iteration 1193, loss = 0.01673452\n",
      "Iteration 1194, loss = 0.01667138\n",
      "Iteration 1195, loss = 0.01661426\n",
      "Iteration 1196, loss = 0.01656016\n",
      "Iteration 1197, loss = 0.01651897\n",
      "Iteration 1198, loss = 0.01645000\n",
      "Iteration 1199, loss = 0.01640665\n",
      "Iteration 1200, loss = 0.01635399\n",
      "Iteration 1201, loss = 0.01627478\n",
      "Iteration 1202, loss = 0.01622004\n",
      "Iteration 1203, loss = 0.01618299\n",
      "Iteration 1204, loss = 0.01611582\n",
      "Iteration 1205, loss = 0.01607356\n",
      "Iteration 1206, loss = 0.01600376\n",
      "Iteration 1207, loss = 0.01595769\n",
      "Iteration 1208, loss = 0.01593568\n",
      "Iteration 1209, loss = 0.01585073\n",
      "Iteration 1210, loss = 0.01579714\n",
      "Iteration 1211, loss = 0.01573142\n",
      "Iteration 1212, loss = 0.01568956\n",
      "Iteration 1213, loss = 0.01564776\n",
      "Iteration 1214, loss = 0.01558860\n",
      "Iteration 1215, loss = 0.01551490\n",
      "Iteration 1216, loss = 0.01548594\n",
      "Iteration 1217, loss = 0.01543678\n",
      "Iteration 1218, loss = 0.01538638\n",
      "Iteration 1219, loss = 0.01533761\n",
      "Iteration 1220, loss = 0.01527557\n",
      "Iteration 1221, loss = 0.01522595\n",
      "Iteration 1222, loss = 0.01517057\n",
      "Iteration 1223, loss = 0.01513386\n",
      "Iteration 1224, loss = 0.01507742\n",
      "Iteration 1225, loss = 0.01502725\n",
      "Iteration 1226, loss = 0.01498822\n",
      "Iteration 1227, loss = 0.01494245\n",
      "Iteration 1228, loss = 0.01490084\n",
      "Iteration 1229, loss = 0.01486498\n",
      "Iteration 1230, loss = 0.01480405\n",
      "Iteration 1231, loss = 0.01476657\n",
      "Iteration 1232, loss = 0.01469953\n",
      "Iteration 1233, loss = 0.01466147\n",
      "Iteration 1234, loss = 0.01461780\n",
      "Iteration 1235, loss = 0.01456659\n",
      "Iteration 1236, loss = 0.01451135\n",
      "Iteration 1237, loss = 0.01446196\n",
      "Iteration 1238, loss = 0.01440936\n",
      "Iteration 1239, loss = 0.01439958\n",
      "Iteration 1240, loss = 0.01437952\n",
      "Iteration 1241, loss = 0.01431959\n",
      "Iteration 1242, loss = 0.01423462\n",
      "Iteration 1243, loss = 0.01418651\n",
      "Iteration 1244, loss = 0.01413756\n",
      "Iteration 1245, loss = 0.01410966\n",
      "Iteration 1246, loss = 0.01405587\n",
      "Iteration 1247, loss = 0.01401163\n",
      "Iteration 1248, loss = 0.01397837\n",
      "Iteration 1249, loss = 0.01394142\n",
      "Iteration 1250, loss = 0.01385909\n",
      "Iteration 1251, loss = 0.01390170\n",
      "Iteration 1252, loss = 0.01380763\n",
      "Iteration 1253, loss = 0.01377293\n",
      "Iteration 1254, loss = 0.01374139\n",
      "Iteration 1255, loss = 0.01366817\n",
      "Iteration 1256, loss = 0.01362763\n",
      "Iteration 1257, loss = 0.01358752\n",
      "Iteration 1258, loss = 0.01354691\n",
      "Iteration 1259, loss = 0.01350002\n",
      "Iteration 1260, loss = 0.01347685\n",
      "Iteration 1261, loss = 0.01342234\n",
      "Iteration 1262, loss = 0.01337105\n",
      "Iteration 1263, loss = 0.01332563\n",
      "Iteration 1264, loss = 0.01332985\n",
      "Iteration 1265, loss = 0.01325418\n",
      "Iteration 1266, loss = 0.01320940\n",
      "Iteration 1267, loss = 0.01318304\n",
      "Iteration 1268, loss = 0.01313142\n",
      "Iteration 1269, loss = 0.01309538\n",
      "Iteration 1270, loss = 0.01305674\n",
      "Iteration 1271, loss = 0.01300659\n",
      "Iteration 1272, loss = 0.01297985\n",
      "Iteration 1273, loss = 0.01292311\n",
      "Iteration 1274, loss = 0.01290806\n",
      "Iteration 1275, loss = 0.01286221\n",
      "Iteration 1276, loss = 0.01283667\n",
      "Iteration 1277, loss = 0.01277469\n",
      "Iteration 1278, loss = 0.01275024\n",
      "Iteration 1279, loss = 0.01268661\n",
      "Iteration 1280, loss = 0.01268289\n",
      "Iteration 1281, loss = 0.01263963\n",
      "Iteration 1282, loss = 0.01259936\n",
      "Iteration 1283, loss = 0.01255291\n",
      "Iteration 1284, loss = 0.01250636\n",
      "Iteration 1285, loss = 0.01247202\n",
      "Iteration 1286, loss = 0.01243447\n",
      "Iteration 1287, loss = 0.01241684\n",
      "Iteration 1288, loss = 0.01237372\n",
      "Iteration 1289, loss = 0.01232831\n",
      "Iteration 1290, loss = 0.01229819\n",
      "Iteration 1291, loss = 0.01226397\n",
      "Iteration 1292, loss = 0.01220331\n",
      "Iteration 1293, loss = 0.01217829\n",
      "Iteration 1294, loss = 0.01212000\n",
      "Iteration 1295, loss = 0.01209653\n",
      "Iteration 1296, loss = 0.01204063\n",
      "Iteration 1297, loss = 0.01201205\n",
      "Iteration 1298, loss = 0.01199126\n",
      "Iteration 1299, loss = 0.01193202\n",
      "Iteration 1300, loss = 0.01190915\n",
      "Iteration 1301, loss = 0.01189531\n",
      "Iteration 1302, loss = 0.01186697\n",
      "Iteration 1303, loss = 0.01181963\n",
      "Iteration 1304, loss = 0.01176410\n",
      "Iteration 1305, loss = 0.01173625\n",
      "Iteration 1306, loss = 0.01171191\n",
      "Iteration 1307, loss = 0.01167664\n",
      "Iteration 1308, loss = 0.01166946\n",
      "Iteration 1309, loss = 0.01159641\n",
      "Iteration 1310, loss = 0.01155874\n",
      "Iteration 1311, loss = 0.01151880\n",
      "Iteration 1312, loss = 0.01149633\n",
      "Iteration 1313, loss = 0.01148395\n",
      "Iteration 1314, loss = 0.01144104\n",
      "Iteration 1315, loss = 0.01142193\n",
      "Iteration 1316, loss = 0.01136101\n",
      "Iteration 1317, loss = 0.01134076\n",
      "Iteration 1318, loss = 0.01128714\n",
      "Iteration 1319, loss = 0.01125417\n",
      "Iteration 1320, loss = 0.01122460\n",
      "Iteration 1321, loss = 0.01121870\n",
      "Iteration 1322, loss = 0.01117781\n",
      "Iteration 1323, loss = 0.01112318\n",
      "Iteration 1324, loss = 0.01112497\n",
      "Iteration 1325, loss = 0.01108171\n",
      "Iteration 1326, loss = 0.01104340\n",
      "Iteration 1327, loss = 0.01102891\n",
      "Iteration 1328, loss = 0.01099819\n",
      "Iteration 1329, loss = 0.01095969\n",
      "Iteration 1330, loss = 0.01090887\n",
      "Iteration 1331, loss = 0.01088289\n",
      "Iteration 1332, loss = 0.01083931\n",
      "Iteration 1333, loss = 0.01081650\n",
      "Iteration 1334, loss = 0.01078327\n",
      "Iteration 1335, loss = 0.01079455\n",
      "Iteration 1336, loss = 0.01072387\n",
      "Iteration 1337, loss = 0.01071366\n",
      "Iteration 1338, loss = 0.01065612\n",
      "Iteration 1339, loss = 0.01063490\n",
      "Iteration 1340, loss = 0.01060120\n",
      "Iteration 1341, loss = 0.01060736\n",
      "Iteration 1342, loss = 0.01053271\n",
      "Iteration 1343, loss = 0.01052528\n",
      "Iteration 1344, loss = 0.01049148\n",
      "Iteration 1345, loss = 0.01045996\n",
      "Iteration 1346, loss = 0.01041785\n",
      "Iteration 1347, loss = 0.01037988\n",
      "Iteration 1348, loss = 0.01037329\n",
      "Iteration 1349, loss = 0.01033928\n",
      "Iteration 1350, loss = 0.01030043\n",
      "Iteration 1351, loss = 0.01033905\n",
      "Iteration 1352, loss = 0.01025385\n",
      "Iteration 1353, loss = 0.01022764\n",
      "Iteration 1354, loss = 0.01020281\n",
      "Iteration 1355, loss = 0.01016748\n",
      "Iteration 1356, loss = 0.01013242\n",
      "Iteration 1357, loss = 0.01014005\n",
      "Iteration 1358, loss = 0.01007623\n",
      "Iteration 1359, loss = 0.01007313\n",
      "Iteration 1360, loss = 0.01002032\n",
      "Iteration 1361, loss = 0.00998479\n",
      "Iteration 1362, loss = 0.00997079\n",
      "Iteration 1363, loss = 0.00994450\n",
      "Iteration 1364, loss = 0.00993581\n",
      "Iteration 1365, loss = 0.00991198\n",
      "Iteration 1366, loss = 0.00987902\n",
      "Iteration 1367, loss = 0.00983999\n",
      "Iteration 1368, loss = 0.00982409\n",
      "Iteration 1369, loss = 0.00977858\n",
      "Iteration 1370, loss = 0.00975028\n",
      "Iteration 1371, loss = 0.00972700\n",
      "Iteration 1372, loss = 0.00971961\n",
      "Iteration 1373, loss = 0.00967731\n",
      "Iteration 1374, loss = 0.00966779\n",
      "Iteration 1375, loss = 0.00963575\n",
      "Iteration 1376, loss = 0.00962711\n",
      "Iteration 1377, loss = 0.00958421\n",
      "Iteration 1378, loss = 0.00955971\n",
      "Iteration 1379, loss = 0.00952664\n",
      "Iteration 1380, loss = 0.00949479\n",
      "Iteration 1381, loss = 0.00948678\n",
      "Iteration 1382, loss = 0.00947018\n",
      "Iteration 1383, loss = 0.00942681\n",
      "Iteration 1384, loss = 0.00946754\n",
      "Iteration 1385, loss = 0.00936449\n",
      "Iteration 1386, loss = 0.00942365\n",
      "Iteration 1387, loss = 0.00932670\n",
      "Iteration 1388, loss = 0.00930681\n",
      "Iteration 1389, loss = 0.00929979\n",
      "Iteration 1390, loss = 0.00928604\n",
      "Iteration 1391, loss = 0.00925632\n",
      "Iteration 1392, loss = 0.00921056\n",
      "Iteration 1393, loss = 0.00921877\n",
      "Iteration 1394, loss = 0.00915761\n",
      "Iteration 1395, loss = 0.00914545\n",
      "Iteration 1396, loss = 0.00910421\n",
      "Iteration 1397, loss = 0.00908957\n",
      "Iteration 1398, loss = 0.00908195\n",
      "Iteration 1399, loss = 0.00904601\n",
      "Iteration 1400, loss = 0.00901257\n",
      "Iteration 1401, loss = 0.00899829\n",
      "Iteration 1402, loss = 0.00896625\n",
      "Iteration 1403, loss = 0.00894765\n",
      "Iteration 1404, loss = 0.00894508\n",
      "Iteration 1405, loss = 0.00890240\n",
      "Iteration 1406, loss = 0.00888193\n",
      "Iteration 1407, loss = 0.00886193\n",
      "Iteration 1408, loss = 0.00882090\n",
      "Iteration 1409, loss = 0.00881581\n",
      "Iteration 1410, loss = 0.00881362\n",
      "Iteration 1411, loss = 0.00875855\n",
      "Iteration 1412, loss = 0.00875697\n",
      "Iteration 1413, loss = 0.00874779\n",
      "Iteration 1414, loss = 0.00870366\n",
      "Iteration 1415, loss = 0.00869541\n",
      "Iteration 1416, loss = 0.00868972\n",
      "Iteration 1417, loss = 0.00864584\n",
      "Iteration 1418, loss = 0.00861484\n",
      "Iteration 1419, loss = 0.00859689\n",
      "Iteration 1420, loss = 0.00857407\n",
      "Iteration 1421, loss = 0.00854128\n",
      "Iteration 1422, loss = 0.00852843\n",
      "Iteration 1423, loss = 0.00855277\n",
      "Iteration 1424, loss = 0.00847480\n",
      "Iteration 1425, loss = 0.00845437\n",
      "Iteration 1426, loss = 0.00844275\n",
      "Iteration 1427, loss = 0.00843803\n",
      "Iteration 1428, loss = 0.00840192\n",
      "Iteration 1429, loss = 0.00837194\n",
      "Iteration 1430, loss = 0.00834838\n",
      "Iteration 1431, loss = 0.00833438\n",
      "Iteration 1432, loss = 0.00833639\n",
      "Iteration 1433, loss = 0.00831753\n",
      "Iteration 1434, loss = 0.00826767\n",
      "Iteration 1435, loss = 0.00825412\n",
      "Iteration 1436, loss = 0.00822386\n",
      "Iteration 1437, loss = 0.00822325\n",
      "Iteration 1438, loss = 0.00820572\n",
      "Iteration 1439, loss = 0.00820816\n",
      "Iteration 1440, loss = 0.00818886\n",
      "Iteration 1441, loss = 0.00819322\n",
      "Iteration 1442, loss = 0.00813486\n",
      "Iteration 1443, loss = 0.00811509\n",
      "Iteration 1444, loss = 0.00806762\n",
      "Iteration 1445, loss = 0.00805737\n",
      "Iteration 1446, loss = 0.00803570\n",
      "Iteration 1447, loss = 0.00803512\n",
      "Iteration 1448, loss = 0.00800515\n",
      "Iteration 1449, loss = 0.00798509\n",
      "Iteration 1450, loss = 0.00796717\n",
      "Iteration 1451, loss = 0.00793970\n",
      "Iteration 1452, loss = 0.00791439\n",
      "Iteration 1453, loss = 0.00790574\n",
      "Iteration 1454, loss = 0.00787056\n",
      "Iteration 1455, loss = 0.00787982\n",
      "Iteration 1456, loss = 0.00783353\n",
      "Iteration 1457, loss = 0.00781607\n",
      "Iteration 1458, loss = 0.00779297\n",
      "Iteration 1459, loss = 0.00778957\n",
      "Iteration 1460, loss = 0.00777006\n",
      "Iteration 1461, loss = 0.00775542\n",
      "Iteration 1462, loss = 0.00773646\n",
      "Iteration 1463, loss = 0.00772490\n",
      "Iteration 1464, loss = 0.00772630\n",
      "Iteration 1465, loss = 0.00769007\n",
      "Iteration 1466, loss = 0.00765756\n",
      "Iteration 1467, loss = 0.00764161\n",
      "Iteration 1468, loss = 0.00762802\n",
      "Iteration 1469, loss = 0.00761338\n",
      "Iteration 1470, loss = 0.00759279\n",
      "Iteration 1471, loss = 0.00756962\n",
      "Iteration 1472, loss = 0.00754813\n",
      "Iteration 1473, loss = 0.00752823\n",
      "Iteration 1474, loss = 0.00750406\n",
      "Iteration 1475, loss = 0.00748631\n",
      "Iteration 1476, loss = 0.00748149\n",
      "Iteration 1477, loss = 0.00747607\n",
      "Iteration 1478, loss = 0.00745559\n",
      "Iteration 1479, loss = 0.00745972\n",
      "Iteration 1480, loss = 0.00743307\n",
      "Iteration 1481, loss = 0.00738710\n",
      "Iteration 1482, loss = 0.00738498\n",
      "Iteration 1483, loss = 0.00736357\n",
      "Iteration 1484, loss = 0.00734054\n",
      "Iteration 1485, loss = 0.00732122\n",
      "Iteration 1486, loss = 0.00730332\n",
      "Iteration 1487, loss = 0.00730521\n",
      "Iteration 1488, loss = 0.00728606\n",
      "Iteration 1489, loss = 0.00724387\n",
      "Iteration 1490, loss = 0.00726184\n",
      "Iteration 1491, loss = 0.00726856\n",
      "Iteration 1492, loss = 0.00719751\n",
      "Iteration 1493, loss = 0.00720586\n",
      "Iteration 1494, loss = 0.00718138\n",
      "Iteration 1495, loss = 0.00717313\n",
      "Iteration 1496, loss = 0.00716546\n",
      "Iteration 1497, loss = 0.00713881\n",
      "Iteration 1498, loss = 0.00709986\n",
      "Iteration 1499, loss = 0.00708927\n",
      "Iteration 1500, loss = 0.00709022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-18 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-18 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-18 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-18 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-18 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-18 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-18 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-18 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-18 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-18 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-18 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-18 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-18 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-18 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-18 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-18 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-18 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-18 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-18 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-18 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-18 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-18 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-18 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-18 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-18 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-18 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-18 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-18 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-18 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-18 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-18 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-18 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-18 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-18 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-18 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-18 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-18 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-18 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-18 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-18 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-18 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-18 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-18 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-18\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-06, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" checked><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MLPClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-06, verbose=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-06, verbose=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3 --> 2 --> 2 ---> 1\n",
    "rede_neural_credit = MLPClassifier(max_iter=1500, verbose = True, tol = 0.000001, solver = 'adam', activation = 'relu', hidden_layer_sizes = (2, 2))\n",
    "rede_neural_credit.fit(x_credit_treinamento, y_credit_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "06f9b02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = rede_neural_credit.predict(x_credit_teste)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "46ebe56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_credit_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7b0906a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy_score(y_credit_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "66e0f023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "cm = ConfusionMatrix(rede_neural_credit)\n",
    "cm.fit(x_credit_treinamento, y_credit_treinamento)\n",
    "cm.score(x_credit_teste, y_credit_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2bfd7045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.98      1.00      0.99        64\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       0.99      1.00      1.00       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_teste, previsoes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0b37e2",
   "metadata": {},
   "source": [
    "## Base Censu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b7ceb80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('census.pkl', 'rb') as f:\n",
    "    x_census_treinamento, y_census_treinamento, x_census_teste, y_census_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bba43173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27676, 108), (27676,))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_census_treinamento.shape, y_census_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c15fb198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4885, 108), (4885,))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_census_teste.shape, y_census_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "689bbee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.37592615\n",
      "Iteration 2, loss = 0.32436659\n",
      "Iteration 3, loss = 0.31256926\n",
      "Iteration 4, loss = 0.30572866\n",
      "Iteration 5, loss = 0.30234719\n",
      "Iteration 6, loss = 0.29674283\n",
      "Iteration 7, loss = 0.29341097\n",
      "Iteration 8, loss = 0.29100269\n",
      "Iteration 9, loss = 0.28854179\n",
      "Iteration 10, loss = 0.28679043\n",
      "Iteration 11, loss = 0.28433034\n",
      "Iteration 12, loss = 0.28230955\n",
      "Iteration 13, loss = 0.27965135\n",
      "Iteration 14, loss = 0.27795492\n",
      "Iteration 15, loss = 0.27718253\n",
      "Iteration 16, loss = 0.27460572\n",
      "Iteration 17, loss = 0.27340710\n",
      "Iteration 18, loss = 0.27157972\n",
      "Iteration 19, loss = 0.26961657\n",
      "Iteration 20, loss = 0.26831947\n",
      "Iteration 21, loss = 0.26680829\n",
      "Iteration 22, loss = 0.26501580\n",
      "Iteration 23, loss = 0.26392115\n",
      "Iteration 24, loss = 0.26194529\n",
      "Iteration 25, loss = 0.26083988\n",
      "Iteration 26, loss = 0.25962534\n",
      "Iteration 27, loss = 0.25845120\n",
      "Iteration 28, loss = 0.25700433\n",
      "Iteration 29, loss = 0.25633580\n",
      "Iteration 30, loss = 0.25393080\n",
      "Iteration 31, loss = 0.25254522\n",
      "Iteration 32, loss = 0.25177607\n",
      "Iteration 33, loss = 0.24972762\n",
      "Iteration 34, loss = 0.24908188\n",
      "Iteration 35, loss = 0.24724081\n",
      "Iteration 36, loss = 0.24635559\n",
      "Iteration 37, loss = 0.24574184\n",
      "Iteration 38, loss = 0.24400185\n",
      "Iteration 39, loss = 0.24325945\n",
      "Iteration 40, loss = 0.24178606\n",
      "Iteration 41, loss = 0.23974206\n",
      "Iteration 42, loss = 0.23968647\n",
      "Iteration 43, loss = 0.23838337\n",
      "Iteration 44, loss = 0.23708160\n",
      "Iteration 45, loss = 0.23588531\n",
      "Iteration 46, loss = 0.23451088\n",
      "Iteration 47, loss = 0.23451694\n",
      "Iteration 48, loss = 0.23324131\n",
      "Iteration 49, loss = 0.23337480\n",
      "Iteration 50, loss = 0.23226406\n",
      "Iteration 51, loss = 0.23026183\n",
      "Iteration 52, loss = 0.22949288\n",
      "Iteration 53, loss = 0.22760618\n",
      "Iteration 54, loss = 0.22739131\n",
      "Iteration 55, loss = 0.22612683\n",
      "Iteration 56, loss = 0.22578389\n",
      "Iteration 57, loss = 0.22531026\n",
      "Iteration 58, loss = 0.22473798\n",
      "Iteration 59, loss = 0.22202780\n",
      "Iteration 60, loss = 0.22231933\n",
      "Iteration 61, loss = 0.22147489\n",
      "Iteration 62, loss = 0.22277411\n",
      "Iteration 63, loss = 0.22041585\n",
      "Iteration 64, loss = 0.21919007\n",
      "Iteration 65, loss = 0.21738822\n",
      "Iteration 66, loss = 0.21736632\n",
      "Iteration 67, loss = 0.21722021\n",
      "Iteration 68, loss = 0.21535322\n",
      "Iteration 69, loss = 0.21518969\n",
      "Iteration 70, loss = 0.21537343\n",
      "Iteration 71, loss = 0.21467933\n",
      "Iteration 72, loss = 0.21290958\n",
      "Iteration 73, loss = 0.21257462\n",
      "Iteration 74, loss = 0.21218363\n",
      "Iteration 75, loss = 0.21104587\n",
      "Iteration 76, loss = 0.20979825\n",
      "Iteration 77, loss = 0.20924630\n",
      "Iteration 78, loss = 0.20893576\n",
      "Iteration 79, loss = 0.20789032\n",
      "Iteration 80, loss = 0.20706041\n",
      "Iteration 81, loss = 0.20744991\n",
      "Iteration 82, loss = 0.20680597\n",
      "Iteration 83, loss = 0.20482760\n",
      "Iteration 84, loss = 0.20509925\n",
      "Iteration 85, loss = 0.20472142\n",
      "Iteration 86, loss = 0.20342017\n",
      "Iteration 87, loss = 0.20213493\n",
      "Iteration 88, loss = 0.20352058\n",
      "Iteration 89, loss = 0.20167145\n",
      "Iteration 90, loss = 0.20196050\n",
      "Iteration 91, loss = 0.20157421\n",
      "Iteration 92, loss = 0.20176588\n",
      "Iteration 93, loss = 0.19994561\n",
      "Iteration 94, loss = 0.19961810\n",
      "Iteration 95, loss = 0.19874282\n",
      "Iteration 96, loss = 0.19812213\n",
      "Iteration 97, loss = 0.19722371\n",
      "Iteration 98, loss = 0.19680758\n",
      "Iteration 99, loss = 0.19682067\n",
      "Iteration 100, loss = 0.19615238\n",
      "Iteration 101, loss = 0.19502239\n",
      "Iteration 102, loss = 0.19467543\n",
      "Iteration 103, loss = 0.19467695\n",
      "Iteration 104, loss = 0.19450502\n",
      "Iteration 105, loss = 0.19335335\n",
      "Iteration 106, loss = 0.19323971\n",
      "Iteration 107, loss = 0.19252569\n",
      "Iteration 108, loss = 0.19244656\n",
      "Iteration 109, loss = 0.19210250\n",
      "Iteration 110, loss = 0.19060615\n",
      "Iteration 111, loss = 0.19032182\n",
      "Iteration 112, loss = 0.19166044\n",
      "Iteration 113, loss = 0.19044122\n",
      "Iteration 114, loss = 0.18931993\n",
      "Iteration 115, loss = 0.18905574\n",
      "Iteration 116, loss = 0.18949762\n",
      "Iteration 117, loss = 0.18833135\n",
      "Iteration 118, loss = 0.18694176\n",
      "Iteration 119, loss = 0.18749470\n",
      "Iteration 120, loss = 0.18626924\n",
      "Iteration 121, loss = 0.18733702\n",
      "Iteration 122, loss = 0.18531377\n",
      "Iteration 123, loss = 0.18588632\n",
      "Iteration 124, loss = 0.18466486\n",
      "Iteration 125, loss = 0.18479124\n",
      "Iteration 126, loss = 0.18430000\n",
      "Iteration 127, loss = 0.18461420\n",
      "Iteration 128, loss = 0.18294043\n",
      "Iteration 129, loss = 0.18305738\n",
      "Iteration 130, loss = 0.18231668\n",
      "Iteration 131, loss = 0.18181953\n",
      "Iteration 132, loss = 0.18072109\n",
      "Iteration 133, loss = 0.18354223\n",
      "Iteration 134, loss = 0.18114134\n",
      "Iteration 135, loss = 0.18141675\n",
      "Iteration 136, loss = 0.18042734\n",
      "Iteration 137, loss = 0.17891732\n",
      "Iteration 138, loss = 0.18065721\n",
      "Iteration 139, loss = 0.18008191\n",
      "Iteration 140, loss = 0.18083530\n",
      "Iteration 141, loss = 0.17995649\n",
      "Iteration 142, loss = 0.17860632\n",
      "Iteration 143, loss = 0.17777300\n",
      "Iteration 144, loss = 0.17586304\n",
      "Iteration 145, loss = 0.17692925\n",
      "Iteration 146, loss = 0.17738591\n",
      "Iteration 147, loss = 0.17689341\n",
      "Iteration 148, loss = 0.17630799\n",
      "Iteration 149, loss = 0.17497428\n",
      "Iteration 150, loss = 0.17554879\n",
      "Iteration 151, loss = 0.17474049\n",
      "Iteration 152, loss = 0.17530346\n",
      "Iteration 153, loss = 0.17449218\n",
      "Iteration 154, loss = 0.17475089\n",
      "Iteration 155, loss = 0.17513386\n",
      "Iteration 156, loss = 0.17277786\n",
      "Iteration 157, loss = 0.17372654\n",
      "Iteration 158, loss = 0.17260199\n",
      "Iteration 159, loss = 0.17125487\n",
      "Iteration 160, loss = 0.17266785\n",
      "Iteration 161, loss = 0.17317135\n",
      "Iteration 162, loss = 0.17239158\n",
      "Iteration 163, loss = 0.17186814\n",
      "Iteration 164, loss = 0.17030733\n",
      "Iteration 165, loss = 0.17217483\n",
      "Iteration 166, loss = 0.17149053\n",
      "Iteration 167, loss = 0.16941277\n",
      "Iteration 168, loss = 0.16960580\n",
      "Iteration 169, loss = 0.17124712\n",
      "Iteration 170, loss = 0.16945889\n",
      "Iteration 171, loss = 0.17013195\n",
      "Iteration 172, loss = 0.16999356\n",
      "Iteration 173, loss = 0.16920291\n",
      "Iteration 174, loss = 0.16934384\n",
      "Iteration 175, loss = 0.16885789\n",
      "Iteration 176, loss = 0.16800238\n",
      "Iteration 177, loss = 0.16924367\n",
      "Iteration 178, loss = 0.16859404\n",
      "Iteration 179, loss = 0.16886787\n",
      "Iteration 180, loss = 0.16730220\n",
      "Iteration 181, loss = 0.16817253\n",
      "Iteration 182, loss = 0.16651351\n",
      "Iteration 183, loss = 0.16683416\n",
      "Iteration 184, loss = 0.16625249\n",
      "Iteration 185, loss = 0.16659099\n",
      "Iteration 186, loss = 0.16650438\n",
      "Iteration 187, loss = 0.16616837\n",
      "Iteration 188, loss = 0.16508607\n",
      "Iteration 189, loss = 0.16483955\n",
      "Iteration 190, loss = 0.16552472\n",
      "Iteration 191, loss = 0.16486886\n",
      "Iteration 192, loss = 0.16505179\n",
      "Iteration 193, loss = 0.16298713\n",
      "Iteration 194, loss = 0.16445369\n",
      "Iteration 195, loss = 0.16403742\n",
      "Iteration 196, loss = 0.16364332\n",
      "Iteration 197, loss = 0.16332607\n",
      "Iteration 198, loss = 0.16388615\n",
      "Iteration 199, loss = 0.16239394\n",
      "Iteration 200, loss = 0.16198888\n",
      "Iteration 201, loss = 0.16164845\n",
      "Iteration 202, loss = 0.16270722\n",
      "Iteration 203, loss = 0.16238330\n",
      "Iteration 204, loss = 0.16141409\n",
      "Iteration 205, loss = 0.16214617\n",
      "Iteration 206, loss = 0.16359731\n",
      "Iteration 207, loss = 0.16283114\n",
      "Iteration 208, loss = 0.16344234\n",
      "Iteration 209, loss = 0.16110433\n",
      "Iteration 210, loss = 0.16125693\n",
      "Iteration 211, loss = 0.15980954\n",
      "Iteration 212, loss = 0.16011227\n",
      "Iteration 213, loss = 0.15970915\n",
      "Iteration 214, loss = 0.16059764\n",
      "Iteration 215, loss = 0.16063218\n",
      "Iteration 216, loss = 0.15940283\n",
      "Iteration 217, loss = 0.15937404\n",
      "Iteration 218, loss = 0.15942962\n",
      "Iteration 219, loss = 0.15910587\n",
      "Iteration 220, loss = 0.15895890\n",
      "Iteration 221, loss = 0.15947379\n",
      "Iteration 222, loss = 0.15823421\n",
      "Iteration 223, loss = 0.15787554\n",
      "Iteration 224, loss = 0.15973243\n",
      "Iteration 225, loss = 0.15670383\n",
      "Iteration 226, loss = 0.15762491\n",
      "Iteration 227, loss = 0.15917252\n",
      "Iteration 228, loss = 0.15796677\n",
      "Iteration 229, loss = 0.15696889\n",
      "Iteration 230, loss = 0.15829404\n",
      "Iteration 231, loss = 0.15765514\n",
      "Iteration 232, loss = 0.15656933\n",
      "Iteration 233, loss = 0.15517358\n",
      "Iteration 234, loss = 0.15654334\n",
      "Iteration 235, loss = 0.15520376\n",
      "Iteration 236, loss = 0.15610443\n",
      "Iteration 237, loss = 0.15440665\n",
      "Iteration 238, loss = 0.15656837\n",
      "Iteration 239, loss = 0.15579618\n",
      "Iteration 240, loss = 0.15698543\n",
      "Iteration 241, loss = 0.15578652\n",
      "Iteration 242, loss = 0.15391385\n",
      "Iteration 243, loss = 0.15391325\n",
      "Iteration 244, loss = 0.15421085\n",
      "Iteration 245, loss = 0.15241180\n",
      "Iteration 246, loss = 0.15401219\n",
      "Iteration 247, loss = 0.15291182\n",
      "Iteration 248, loss = 0.15258780\n",
      "Iteration 249, loss = 0.15364035\n",
      "Iteration 250, loss = 0.15400777\n",
      "Iteration 251, loss = 0.15348338\n",
      "Iteration 252, loss = 0.15326527\n",
      "Iteration 253, loss = 0.15393924\n",
      "Iteration 254, loss = 0.15293671\n",
      "Iteration 255, loss = 0.15153666\n",
      "Iteration 256, loss = 0.15281993\n",
      "Iteration 257, loss = 0.15184675\n",
      "Iteration 258, loss = 0.15222074\n",
      "Iteration 259, loss = 0.15014160\n",
      "Iteration 260, loss = 0.15196485\n",
      "Iteration 261, loss = 0.15110253\n",
      "Iteration 262, loss = 0.14972663\n",
      "Iteration 263, loss = 0.15036020\n",
      "Iteration 264, loss = 0.15037980\n",
      "Iteration 265, loss = 0.14870316\n",
      "Iteration 266, loss = 0.15036700\n",
      "Iteration 267, loss = 0.15085999\n",
      "Iteration 268, loss = 0.15009146\n",
      "Iteration 269, loss = 0.15045230\n",
      "Iteration 270, loss = 0.14936320\n",
      "Iteration 271, loss = 0.15206902\n",
      "Iteration 272, loss = 0.15017531\n",
      "Iteration 273, loss = 0.15022887\n",
      "Iteration 274, loss = 0.15045891\n",
      "Iteration 275, loss = 0.15046865\n",
      "Iteration 276, loss = 0.15011347\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-20 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-20 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-20 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-20 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-20 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-20 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-20 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-20 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-20 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-20 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-20 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-20 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-20 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-20 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-20 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-20 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-20 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-20 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-20 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-20 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-20 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-20 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-20 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-20 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-20 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-20 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-20 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-20 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-20 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-20 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-20 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-20 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-20 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-20 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-20 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-20 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-20 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-20 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-20 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-20 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-20 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-20 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-20 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-20\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-06,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" checked><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MLPClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-06,\n",
       "              verbose=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-06,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(108 + 1)/2 = 54.5 = 55\n",
    "#108 --> 55 --> 55 --> 1\n",
    "rede_neural_census = MLPClassifier(verbose = True, max_iter = 1000, tol = 0.000001, hidden_layer_sizes = (55, 55))\n",
    "rede_neural_census.fit(x_census_treinamento, y_census_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4aa98f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' >50K'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = rede_neural_census.predict(x_census_teste)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0753f1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' <=50K'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_census_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "985f126b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8204708290685773"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_census_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4cf9c8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8204708290685773"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(rede_neural_census)\n",
    "cm.fit(x_census_treinamento, y_census_treinamento)\n",
    "cm.score(x_census_teste, y_census_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "34ae3309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.87      0.89      0.88      3693\n",
      "        >50K       0.64      0.60      0.62      1192\n",
      "\n",
      "    accuracy                           0.82      4885\n",
      "   macro avg       0.76      0.75      0.75      4885\n",
      "weighted avg       0.82      0.82      0.82      4885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_census_teste, previsoes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172cc02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
